{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fd8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df= pd.read_csv(r'yassir-ai-market-challenge\\yassir_marekt_data_09_2025 2\\test_new_version.csv')\n",
    "unique = df['user_id'].nunique()\n",
    "df2= pd.read_csv(r'submission_new_3_fixed_22222223_nine_wiw_yousefff.csv')\n",
    "merged= df2[df2['user_id'].isin(df['user_id'])]\n",
    "merged.to_csv('submission_new_3_fixed_22222223_nine_wiw_yousefff2232.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7324bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.15.3)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.5 MB 219.4 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.5 MB 476.3 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 794.9 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.4/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bcf0230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FAST OPTIMIZED REORDER PREDICTION - TARGET MAP 0.5+ in <10 MIN\n",
      "================================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "  Orders: 3,193,704, Order-Products: 31,536,952\n",
      "\n",
      "[3/6] Creating training data...\n",
      "  Sampled 144,346 users for training\n",
      "\n",
      "[2/6] Building features (vectorized)...\n",
      "  User-Product features...\n",
      "  Recency features...\n",
      "  Product features...\n",
      "  User features...\n",
      "  Candidates: 8,695,008\n",
      "  Samples: 8,695,008, Positive: 0.0643\n",
      "\n",
      "[4/6] Training model...\n",
      "  Features: 17, Samples: 8,695,008\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid's auc: 0.819464\n",
      "[100]\tvalid's auc: 0.820656\n",
      "[150]\tvalid's auc: 0.821436\n",
      "[200]\tvalid's auc: 0.822108\n",
      "[250]\tvalid's auc: 0.822596\n",
      "[300]\tvalid's auc: 0.823039\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[300]\tvalid's auc: 0.823039\n",
      "\n",
      "  Best: iter=300, AUC=0.8230\n",
      "\n",
      "[5/6] Predicting...\n",
      "\n",
      "[2/6] Building features (vectorized)...\n",
      "  User-Product features...\n",
      "  Recency features...\n",
      "  Product features...\n",
      "  User features...\n",
      "  Candidates: 4,164,843\n",
      "\n",
      "[6/6] Generating submission...\n",
      "  ✓ Saved 75,486 users\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FAST OPTIMIZED REORDER PREDICTION - TARGET MAP 0.5+ in <10 MIN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "CONFIG = {\n",
    "    'data_path': 'yassir-ai-market-challenge/yassir_marekt_data_09_2025 2/',\n",
    "    'threshold': 0.15,\n",
    "    'sample_frac': 0.7,  # Sample training data for speed\n",
    "}\n",
    "\n",
    "def load_data():\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    base_path = CONFIG['data_path']\n",
    "    \n",
    "    # Load without dtype for columns that might be strings\n",
    "    orders = pd.read_csv(base_path + 'orders_df.csv')\n",
    "    order_products = pd.read_csv(base_path + 'orders_products_df.csv',\n",
    "                                 dtype={'order_id': np.int32, 'product_id': np.int32,\n",
    "                                        'add_to_cart_order': np.int16, 'reordered': np.int8})\n",
    "    products = pd.read_csv(base_path + 'products_df.csv',\n",
    "                          dtype={'product_id': np.int32, 'category_id': np.int16})\n",
    "    user_test = pd.read_csv(r'yassir-ai-market-challenge\\yassir_marekt_data_09_2025 2\\test_new_version.csv',\n",
    "                           dtype={'user_id': np.int32})\n",
    "    \n",
    "    # Convert and optimize orders dtypes\n",
    "    orders['user_id'] = orders['user_id'].astype(np.int32)\n",
    "    orders['order_id'] = orders['order_id'].astype(np.int32)\n",
    "    orders['order_number'] = orders['order_number'].astype(np.int16)\n",
    "    \n",
    "    # Convert day/hour to numeric\n",
    "    if orders['order_day'].dtype == 'object':\n",
    "        day_map = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, \n",
    "                   'Friday': 5, 'Saturday': 6, 'Sunday': 0}\n",
    "        orders['order_day'] = orders['order_day'].map(day_map).fillna(0).astype(np.int8)\n",
    "    else:\n",
    "        orders['order_day'] = orders['order_day'].astype(np.int8)\n",
    "    \n",
    "    if orders['order_hour'].dtype == 'object':\n",
    "        orders['order_hour'] = pd.to_numeric(orders['order_hour'], errors='coerce').fillna(0).astype(np.int8)\n",
    "    else:\n",
    "        orders['order_hour'] = orders['order_hour'].astype(np.int8)\n",
    "    \n",
    "    orders['days_since_last_order'] = orders['days_since_last_order'].astype(np.float32)\n",
    "    \n",
    "    print(f\"  Orders: {len(orders):,}, Order-Products: {len(order_products):,}\")\n",
    "    \n",
    "    return orders, order_products, products, user_test['user_id'].values\n",
    "\n",
    "def build_fast_features(order_products, orders, products):\n",
    "    \"\"\"Build critical features FAST using vectorized operations\"\"\"\n",
    "    print(\"\\n[2/6] Building features (vectorized)...\")\n",
    "    \n",
    "    # Single merge with only needed columns\n",
    "    op = order_products.merge(\n",
    "        orders[['order_id', 'user_id', 'order_number']], \n",
    "        on='order_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Vectorized order age calculation\n",
    "    user_max_order = op.groupby('user_id')['order_number'].transform('max')\n",
    "    op['orders_ago'] = user_max_order - op['order_number']\n",
    "    \n",
    "    print(\"  User-Product features...\")\n",
    "    # Fast aggregation with only critical features\n",
    "    up = op.groupby(['user_id', 'product_id'], sort=False).agg({\n",
    "        'order_id': 'count',\n",
    "        'reordered': 'sum',\n",
    "        'order_number': ['min', 'max'],\n",
    "        'orders_ago': 'min',\n",
    "        'add_to_cart_order': 'mean'\n",
    "    })\n",
    "    \n",
    "    up.columns = ['up_orders', 'up_reorders', 'up_first_order', 'up_last_order',\n",
    "                  'up_orders_since_last', 'up_cart_pos']\n",
    "    up.reset_index(inplace=True)\n",
    "    \n",
    "    # Vectorized feature creation\n",
    "    up['user_total_orders'] = up['user_id'].map(user_max_order.groupby(op['user_id']).first())\n",
    "    up['up_order_rate'] = up['up_orders'] / up['user_total_orders']\n",
    "    up['up_reorder_ratio'] = np.where(up['up_orders'] > 1, \n",
    "                                       up['up_reorders'] / (up['up_orders'] - 1), 0)\n",
    "    \n",
    "    # Fast recency features (most important!)\n",
    "    print(\"  Recency features...\")\n",
    "    for window in [3, 5]:\n",
    "        mask = op['orders_ago'] <= window\n",
    "        recent = op[mask].groupby(['user_id', 'product_id'], sort=False).size()\n",
    "        up = up.merge(recent.rename(f'up_last_{window}'), \n",
    "                     left_on=['user_id', 'product_id'], \n",
    "                     right_index=True, how='left')\n",
    "        up[f'up_last_{window}'] = up[f'up_last_{window}'].fillna(0)\n",
    "    \n",
    "    # Combined recency score\n",
    "    up['up_recency_score'] = up['up_last_3'] * 2 + up['up_last_5']\n",
    "    \n",
    "    print(\"  Product features...\")\n",
    "    # Product stats\n",
    "    prod = op.groupby('product_id', sort=False).agg({\n",
    "        'order_id': 'count',\n",
    "        'reordered': 'mean'\n",
    "    })\n",
    "    prod.columns = ['prod_orders', 'prod_reorder_rate']\n",
    "    \n",
    "    print(\"  User features...\")\n",
    "    # User stats\n",
    "    user = orders.groupby('user_id', sort=False).agg({\n",
    "        'order_id': 'count',\n",
    "        'days_since_last_order': 'mean'\n",
    "    })\n",
    "    user.columns = ['user_orders', 'user_avg_days']\n",
    "    \n",
    "    # User reorder rate\n",
    "    user_reorder = op.groupby('user_id', sort=False)['reordered'].mean()\n",
    "    user['user_reorder_rate'] = user_reorder\n",
    "    \n",
    "    return up, prod, user\n",
    "\n",
    "def create_training_data(order_products, orders, products, sample_frac=0.7):\n",
    "    \"\"\"Create training data with sampling for speed\"\"\"\n",
    "    print(\"\\n[3/6] Creating training data...\")\n",
    "    \n",
    "    # Use last order for validation\n",
    "    user_max = orders.groupby('user_id')['order_number'].max()\n",
    "    orders_with_max = orders.merge(user_max.rename('max_order'), \n",
    "                                   left_on='user_id', right_index=True)\n",
    "    \n",
    "    # Sample users for faster training\n",
    "    train_users = orders_with_max[orders_with_max['order_number'] == orders_with_max['max_order']]['user_id'].unique()\n",
    "    if sample_frac < 1.0:\n",
    "        np.random.seed(42)\n",
    "        train_users = np.random.choice(train_users, \n",
    "                                      size=int(len(train_users) * sample_frac), \n",
    "                                      replace=False)\n",
    "        print(f\"  Sampled {len(train_users):,} users for training\")\n",
    "    \n",
    "    # Get train and prior orders\n",
    "    train_orders = orders_with_max[\n",
    "        (orders_with_max['user_id'].isin(train_users)) & \n",
    "        (orders_with_max['order_number'] == orders_with_max['max_order'])\n",
    "    ]['order_id'].values\n",
    "    \n",
    "    prior_orders = orders_with_max[\n",
    "        (orders_with_max['user_id'].isin(train_users)) & \n",
    "        (orders_with_max['order_number'] < orders_with_max['max_order'])\n",
    "    ]\n",
    "    \n",
    "    # Get labels\n",
    "    train_labels = order_products[order_products['order_id'].isin(train_orders)].copy()\n",
    "    train_labels = train_labels.merge(orders[['order_id', 'user_id']], on='order_id')\n",
    "    train_labels['label'] = 1\n",
    "    \n",
    "    # Build features on prior\n",
    "    up_feat, prod_feat, user_feat = build_fast_features(\n",
    "        order_products[order_products['order_id'].isin(prior_orders['order_id'])],\n",
    "        prior_orders,\n",
    "        products\n",
    "    )\n",
    "    \n",
    "    # Get candidates\n",
    "    candidates = up_feat[['user_id', 'product_id']].copy()\n",
    "    print(f\"  Candidates: {len(candidates):,}\")\n",
    "    \n",
    "    # Fast merge all features\n",
    "    train_df = candidates.merge(up_feat, on=['user_id', 'product_id'], how='left')\n",
    "    train_df = train_df.merge(prod_feat, left_on='product_id', right_index=True, how='left')\n",
    "    train_df = train_df.merge(user_feat, left_on='user_id', right_index=True, how='left')\n",
    "    \n",
    "    # Add labels\n",
    "    train_df = train_df.merge(\n",
    "        train_labels[['user_id', 'product_id', 'label']],\n",
    "        on=['user_id', 'product_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    train_df['label'] = train_df['label'].fillna(0).astype(np.int8)\n",
    "    \n",
    "    # Fill NaN efficiently\n",
    "    for col in train_df.select_dtypes(include=[np.number]).columns:\n",
    "        if train_df[col].isna().any():\n",
    "            train_df[col] = train_df[col].fillna(0)\n",
    "    \n",
    "    print(f\"  Samples: {len(train_df):,}, Positive: {train_df['label'].mean():.4f}\")\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "def train_model(train_df):\n",
    "    \"\"\"Train LightGBM with speed optimizations\"\"\"\n",
    "    print(\"\\n[4/6] Training model...\")\n",
    "    \n",
    "    exclude = ['user_id', 'product_id', 'label']\n",
    "    features = [c for c in train_df.columns if c not in exclude]\n",
    "    \n",
    "    X = train_df[features].values.astype(np.float32)\n",
    "    y = train_df['label'].values\n",
    "    \n",
    "    print(f\"  Features: {len(features)}, Samples: {len(X):,}\")\n",
    "    \n",
    "    # Smaller validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fast parameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'min_child_samples': 100,\n",
    "        'subsample': 0.8,\n",
    "        'subsample_freq': 1,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.05,\n",
    "        'reg_lambda': 0.05,\n",
    "        'verbose': -1,\n",
    "        'force_col_wise': True,\n",
    "        'num_threads': 4,\n",
    "        'max_bin': 255\n",
    "    }\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=True)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train, free_raw_data=True)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=300,\n",
    "        valid_sets=[lgb_val],\n",
    "        valid_names=['valid'],\n",
    "        callbacks=[lgb.early_stopping(30), lgb.log_evaluation(50)]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  Best: iter={model.best_iteration}, AUC={model.best_score['valid']['auc']:.4f}\")\n",
    "    \n",
    "    return model, features\n",
    "\n",
    "def predict_test(model, features, orders, order_products, products, test_users):\n",
    "    \"\"\"Fast prediction\"\"\"\n",
    "    print(\"\\n[5/6] Predicting...\")\n",
    "    \n",
    "    # Build features on full data\n",
    "    up_feat, prod_feat, user_feat = build_fast_features(order_products, orders, products)\n",
    "    \n",
    "    # Filter to test users\n",
    "    test_df = up_feat[up_feat['user_id'].isin(test_users)].copy()\n",
    "    test_df = test_df.merge(prod_feat, left_on='product_id', right_index=True, how='left')\n",
    "    test_df = test_df.merge(user_feat, left_on='user_id', right_index=True, how='left')\n",
    "    \n",
    "    # Fill NaN\n",
    "    for col in test_df.select_dtypes(include=[np.number]).columns:\n",
    "        if test_df[col].isna().any():\n",
    "            test_df[col] = test_df[col].fillna(0)\n",
    "    \n",
    "    print(f\"  Candidates: {len(test_df):,}\")\n",
    "    \n",
    "    # Predict in batches\n",
    "    X_test = test_df[features].values.astype(np.float32)\n",
    "    test_df['prob'] = model.predict(X_test, num_threads=4)\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "def generate_submission(test_df, test_users, threshold, order_products):\n",
    "    \"\"\"Fast submission generation\"\"\"\n",
    "    print(\"\\n[6/6] Generating submission...\")\n",
    "    \n",
    "    # Pre-compute fallback products\n",
    "    fallback = order_products[order_products['reordered']==1]['product_id'].value_counts().head(50).index.tolist()\n",
    "    \n",
    "    # Vectorized submission creation\n",
    "    test_df = test_df.sort_values(['user_id', 'prob'], ascending=[True, False])\n",
    "    \n",
    "    results = []\n",
    "    for user_id in test_users:\n",
    "        user_df = test_df[test_df['user_id'] == user_id].head(20)\n",
    "        \n",
    "        # Take top products above threshold\n",
    "        selected = user_df[user_df['prob'] >= threshold]['product_id'].tolist()[:10]\n",
    "        \n",
    "        # Fill with high probability items\n",
    "        if len(selected) < 5:\n",
    "            additional = user_df[~user_df['product_id'].isin(selected)].head(10)\n",
    "            selected.extend(additional['product_id'].tolist())\n",
    "        \n",
    "        # Fill to 10\n",
    "        for p in fallback:\n",
    "            if len(selected) >= 10:\n",
    "                break\n",
    "            if p not in selected:\n",
    "                selected.append(p)\n",
    "        \n",
    "        results.append(' '.join(map(str, selected[:10])))\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'user_id': test_users,\n",
    "        'products': results\n",
    "    })\n",
    "    \n",
    "    submission = submission.sort_values('user_id')\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(f\"  ✓ Saved {len(submission):,} users\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def main():\n",
    "    orders, order_products, products, test_users = load_data()\n",
    "    \n",
    "    train_df = create_training_data(\n",
    "        order_products, orders, products, \n",
    "        sample_frac=CONFIG['sample_frac']\n",
    "    )\n",
    "    \n",
    "    model, features = train_model(train_df)\n",
    "    \n",
    "    test_df = predict_test(model, features, orders, order_products, products, test_users)\n",
    "    generate_submission(test_df, test_users, CONFIG['threshold'], order_products)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59379ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'product_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv('submission.csv')\n",
    "\n",
    "# Rename the wrong column if it exists\n",
    "df.rename(columns={'products': 'product_id'}, inplace=True)\n",
    "\n",
    "# (Optional) Verify the result\n",
    "print(df.columns)\n",
    "\n",
    "# Save back to CSV\n",
    "df.to_csv('submission_yousef_new_sayi.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
